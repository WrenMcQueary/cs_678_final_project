Loading Dataset...
 === Rationale Level: token === 
==> Token-Level Rationale Features <==
Loading Training Set -- Sample Size = 60120
==> Token-Level Rationale Features <==
Loading Validation Set -- Sample Size = 7515
==> Token-Level Rationale Features <==
Loading Test Set -- Sample Size = 7515
Loading Model...
	-----------------------------------------------------------------------
	 ============= LimitedInk Modeling =============
	 ====== Model Setting: length percent=0.5, seed=1234 ====== 
	 ====== Model Setting: model_kwargs={'continuity_lambda': 0.5, 'sparsity_lambda': 0.3, 'comprehensive_lambda': 0.0001} ====== 
	 ====== Configs Setting: model_kwargs={'data_params': {'task': 'e-xnli', 'model': 'xlm-r', 'batch_size': 3, 'max_seq_length': 512, 'max_query_length': 24, 'max_num_sentences': 20, 'classes': ['contradiction', 'neutral', 'entailment'], 'labels_ids': {'contradiction': 0, 'neutral': 1, 'entailment': 2}, 'truncate': False, 'partial_train': 1.0, 'rationale_level': 'token', 'overwrite_cache': True, 'cached_features_file': 'token_cached_features_file.pt', 'remove_query_input': False}, 'model_params': {'tau': 0.1, 'num_labels': 3, 'model_type': 'xlm-roberta-base', 'dropout': 0.5, 'loss_function': 'limitedink'}, 'train_params': {'epochs': 15, 'lr': 2e-05}, 'model_kwargs': {'continuity_lambda': 0.5, 'sparsity_lambda': 0.3, 'comprehensive_lambda': 0.0001}} ====== 
	-----------------------------------------------------------------------
	-----------------------------------------------------------------------
	 ============= LimitedInk Model Training =============
	-----------------------------------------------------------------------
	 ========================== Epoch: 01 ==========================
	 	batch_idx: 1 - train_loss: 23.67462730407715 - train_acc: 0.3333333333333333 
	 	batch_idx: 501 - train_loss: 31.6589320329373 - train_acc: 0.34094498247282445 
	 	batch_idx: 1001 - train_loss: 31.835323488081134 - train_acc: 0.33811214881269136 
	 	batch_idx: 1501 - train_loss: 31.703133887405954 - train_acc: 0.3379207511452598 
	 	batch_idx: 2001 - train_loss: 31.632200785364763 - train_acc: 0.3371699125577905 
	 	batch_idx: 2501 - train_loss: 31.604361360619325 - train_acc: 0.3371061895430365 
	 	batch_idx: 3001 - train_loss: 31.609360657068777 - train_acc: 0.33751097663003604 
	 	batch_idx: 3501 - train_loss: 31.515639076025884 - train_acc: 0.3379754492778616 
	 	batch_idx: 4001 - train_loss: 31.32193697729399 - train_acc: 0.3384903003868878 
	 	batch_idx: 4501 - train_loss: 31.17923309781291 - train_acc: 0.33879617670530743 
	 	batch_idx: 5001 - train_loss: 31.05480230228826 - train_acc: 0.3388107950141465 
	 	batch_idx: 5501 - train_loss: 30.88001384866864 - train_acc: 0.33866877699862513 
	 	batch_idx: 6001 - train_loss: 30.680193362166893 - train_acc: 0.33840526694576367 
	 	batch_idx: 6501 - train_loss: 30.473961054554465 - train_acc: 0.3382026880151862 
	 	batch_idx: 7001 - train_loss: 30.29195571098987 - train_acc: 0.33800454333108154 
	 	batch_idx: 7501 - train_loss: 30.137245292266897 - train_acc: 0.33786425363461137 
	 	batch_idx: 8001 - train_loss: 29.988129486576973 - train_acc: 0.33776525062140533 
	 	batch_idx: 8501 - train_loss: 29.877189366428478 - train_acc: 0.3376649115988086 
	 	batch_idx: 9001 - train_loss: 29.726499387442093 - train_acc: 0.33758867591049874 
	 	batch_idx: 9501 - train_loss: 29.597457646746445 - train_acc: 0.33746136651115344 
	 	batch_idx: 10001 - train_loss: 29.487912651396147 - train_acc: 0.33739551299753673 
	 	batch_idx: 10501 - train_loss: 29.389788965283888 - train_acc: 0.3373287546641565 
	 	batch_idx: 11001 - train_loss: 29.286227235359753 - train_acc: 0.3372822871980801 
	 	batch_idx: 11501 - train_loss: 29.183940090082924 - train_acc: 0.3372211368078744 
	 	batch_idx: 12001 - train_loss: 29.1169627403162 - train_acc: 0.3371572836623655 
	 	batch_idx: 12501 - train_loss: 29.03660449877938 - train_acc: 0.33708532209615943 
	 	batch_idx: 13001 - train_loss: 28.946571033722197 - train_acc: 0.3369923867429953 
	 	batch_idx: 13501 - train_loss: 28.871456991415645 - train_acc: 0.33689467046680416 
	 	batch_idx: 14001 - train_loss: 28.808611331296966 - train_acc: 0.336784471507784 
	 	batch_idx: 14501 - train_loss: 28.741838410939604 - train_acc: 0.33668029177296993 
	 	batch_idx: 15001 - train_loss: 28.674789580780065 - train_acc: 0.3365667141795207 
	 	batch_idx: 15501 - train_loss: 28.614327198166624 - train_acc: 0.336454397467777 
	 	batch_idx: 16001 - train_loss: 28.547822057958708 - train_acc: 0.3363424157612839 
	 	batch_idx: 16501 - train_loss: 28.503609021716635 - train_acc: 0.3362388538093496 
	 	batch_idx: 17001 - train_loss: 28.44900222155888 - train_acc: 0.33614394495122224 
	 	batch_idx: 17501 - train_loss: 28.414962456203327 - train_acc: 0.3360576815728636 
	 	batch_idx: 18001 - train_loss: 28.376521377495664 - train_acc: 0.3359772646730279 
	 	batch_idx: 18501 - train_loss: 28.34093034396938 - train_acc: 0.33588968228848615 
	 	batch_idx: 19001 - train_loss: 28.297533647806155 - train_acc: 0.3357993261753341 
	 	batch_idx: 19501 - train_loss: 28.268077280024897 - train_acc: 0.3357130187439637 
	 	batch_idx: 20001 - train_loss: 28.229529629075035 - train_acc: 0.3356322857656122 
	-----------------------------------------------------------------------
	| Epoch: 0 | Train Loss: 28.225 
	-----------------------------------------------------------------------
               precision    recall  f1-score   support

contradiction       0.33      0.33      0.33     19815
      neutral       0.33      0.34      0.34     20130
   entailment       0.34      0.33      0.33     20175

     accuracy                           0.33     60120
    macro avg       0.33      0.33      0.33     60120
 weighted avg       0.33      0.33      0.33     60120

	 	batch_idx: 1 - val_loss: 17.52031707763672 - val_acc: 0.6666666666666666 
