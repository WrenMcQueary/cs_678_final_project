{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness check using [CheckList](https://github.com/marcotcr/checklist)\n",
    "\n",
    "Evaluate a LimitedInk DistilBERT model trained on the `movies` dataset.\n",
    "This notebook only tests the identifier (not the classifier), because the identifier is more interesting and is the novel contribution of the LimitedInk paper.\n",
    "\n",
    "Borrows heavily from [this tutorial notebook](https://github.com/marcotcr/checklist/blob/master/notebooks/tutorials/4.%20The%20CheckList%20process.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task and model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model and spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/wren/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "import checklist\n",
    "from checklist.editor import Editor\n",
    "from checklist.perturb import Perturb\n",
    "from checklist.test_types import MFT, INV, DIR\n",
    "from checklist.test_suite import TestSuite\n",
    "from checklist.expect import Expect\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from torch.utils.data import SequentialSampler, Dataset, DataLoader, TensorDataset\n",
    "\n",
    "import sys\n",
    "import spacy\n",
    "import numpy as np\n",
    "processor = spacy.load('en_core_web_sm')\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "state_dict = torch.load('checkpoints/movies/distilbert/token_rationale/length_level_0.5/seed_1234/models/classifier_ckpt_k0.5.pt')\n",
    "for key in list(state_dict.keys()):\n",
    "    state_dict[key[6:]] = state_dict.pop(key)\n",
    "model.load_state_dict(state_dict)\n",
    "# sentiment analysis is a general name in Huggingface to load the pipeline for text classification tasks.\n",
    "# set device=-1 if you don't have a gpu\n",
    "pipe = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, framework=\"pt\", device=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_to_tensor(features, rationale_level=\"token\"):\n",
    "    \"\"\"Borrowed from limitedink/utils/utils_dataloader.py, with possible modifications.\"\"\"\n",
    "    \n",
    "    if rationale_level == \"token\":\n",
    "        \"\"\"\n",
    "        input_ids:        input index;\n",
    "        class_labels:     prediction label;\n",
    "        input_mask:       input token = 1, padding = 0; \n",
    "        evidence_masks:   human annotated token = 1, others = 0;\n",
    "        p_mask:           [CLS] and Paragraph tokens = 0, [SEP] and Query tokens = 1 \n",
    "        \"\"\"\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "        all_class_labels = torch.tensor([f.class_label for f in features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "        all_evidence_masks = torch.tensor([f.evidence_mask for f in features], dtype=torch.long)\n",
    "        all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.long)      \n",
    "\n",
    "        tensorized_dataset = TensorDataset(all_input_ids,\n",
    "                                           all_class_labels, \n",
    "                                           all_input_mask,\n",
    "                                           all_evidence_masks, \n",
    "                                           all_p_mask)\n",
    "\n",
    "    elif rationale_level == \"sentence\":\n",
    "        \"\"\"\n",
    "        input_ids:        input index;\n",
    "        class_labels:     prediction label;\n",
    "        input_mask:       input token = 1, padding = 0;\n",
    "        p_mask:           p_mask only contains sentence membership of the paragrph and is of length 512 - max_quey_len\n",
    "        sentence_starts:  index of sentence start;\n",
    "        sentence_ends:    index of sentence end;\n",
    "        sentence_mask:    input sentence = 1, others = 0; sentence_mask doesn't contain query with (max_query_length)\n",
    "        evidence_masks:   human annotated sentence = 1, others = 0\n",
    "        \"\"\"\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "        all_class_labels = torch.tensor([f.class_label for f in features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "        all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.long)\n",
    "        all_sentence_starts = torch.tensor([f.sentence_starts for f in features], dtype=torch.long)\n",
    "        all_sentence_ends = torch.tensor([f.sentence_ends for f in features], dtype=torch.long)\n",
    "        all_sentence_mask = torch.tensor([f.sentence_mask for f in features], dtype=torch.long)\n",
    "        all_evidence_masks = torch.tensor([f.evidence_mask for f in features], dtype=torch.long)\n",
    "\n",
    "        tensorized_dataset = TensorDataset(all_input_ids,\n",
    "                                           all_class_labels, \n",
    "                                           all_input_mask, \n",
    "                                           all_evidence_masks,\n",
    "                                           all_p_mask, \n",
    "                                           all_sentence_starts, \n",
    "                                           all_sentence_ends, \n",
    "                                           all_sentence_mask\n",
    "                                           )\n",
    "\n",
    "    return tensorized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from limitedink.utils.utils_dataloader import set_seed, load_data\n",
    "from limitedink.utils.utils_dataset import annotations_from_jsonl, load_documents, read_examples, convert_examples_to_features, convert_examples_to_sentence_features\n",
    "import os\n",
    "import pickle\n",
    "from itertools import chain\n",
    "\n",
    "def load_split_data(data_dir, configs, split, seed, tokenizer, partial_train=1.0, rationale_level=\"token\", save_human_annotation=True):\n",
    "    \"\"\"Borrowed from limitedink/utils/utils_dataloader.py, with possible modifications.\"\"\"\n",
    "    set_seed(seed)\n",
    "\n",
    "    cached_features_root = os.path.join(data_dir, rationale_level, split)\n",
    "    if not os.path.exists(cached_features_root):\n",
    "        os.makedirs(cached_features_root)\n",
    "\n",
    "    cached_features_file = os.path.join(cached_features_root, configs['cached_features_file'])    \n",
    "\n",
    "    if os.path.exists(cached_features_file) and not configs['overwrite_cache']:\n",
    "        print(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "        tensorized_dataset = feature_to_tensor(features, rationale_level=rationale_level)\n",
    "        return tensorized_dataset\n",
    "\n",
    "    else:\n",
    "\n",
    "        \"\"\"Step1: Read Annotations\"\"\"\n",
    "        dataset = annotations_from_jsonl(os.path.join(data_dir, split + \".jsonl\"))  # get a list of Annotation\n",
    "        docids = set(e.docid for e in chain.from_iterable(chain.from_iterable(map(lambda ann: ann.evidences, chain(dataset))))) # length=2915\n",
    "        \n",
    "        if \"movies\" in configs[\"task\"]:\n",
    "            docids.add(\"posR_161.txt\")\n",
    "\n",
    "        if \"boolq\" in configs[\"task\"] or \"evidence_inference\" in configs[\"task\"]:\n",
    "            docids  = set([ex.docids[0] for ex in dataset])\n",
    "\n",
    "        if configs[\"task\"] in [\"beer\", \"imdb\", \"twitter\", \"sst\"]: # no human annotations\n",
    "            docids= set([ex.annotation_id for ex in dataset])\n",
    "\n",
    "        if \"e-xnli\" in configs[\"task\"]:\n",
    "            docids = set([ex.annotation_id for ex in dataset])\n",
    "\n",
    "        \"\"\"Step2: Read Full Tokenized Texts\"\"\"\n",
    "        documents = load_documents(data_dir, docids)  # get a list of tokenized docs\n",
    "\n",
    "\n",
    "        \"\"\"Step3: Generate Token-wise Examples\"\"\"\n",
    "        examples = read_examples(configs, data_dir, dataset, documents, split)  # get a list of \"utils.utils_dataset.Example object\"\n",
    "        examples = examples[:int(partial_train * len(examples))]\n",
    "\n",
    "\n",
    "        \"\"\"Step4: Convert Examples to Features\"\"\"\n",
    "\n",
    "        if rationale_level == \"sentence\":\n",
    "            print(\"==> Sentence-Level Rationale Features <==\")\n",
    "            features = convert_examples_to_sentence_features(configs, \n",
    "                                                    examples=examples,\n",
    "                                                    tokenizer=tokenizer,\n",
    "                                                    max_seq_length=configs['max_seq_length'],\n",
    "                                                    max_query_length=configs['max_query_length'])\n",
    "\n",
    "        elif rationale_level == \"token\":\n",
    "            print(\"==> Token-Level Rationale Features <==\")\n",
    "            features = convert_examples_to_features(configs, \n",
    "                                                    examples=examples,\n",
    "                                                    tokenizer=tokenizer,\n",
    "                                                    max_seq_length=configs['max_seq_length'],\n",
    "                                                    max_query_length=configs['max_query_length'])\n",
    "            if save_human_annotation:\n",
    "                human_annotations = {\n",
    "                    \"tokens\": np.array([f.tokens for f in features]),\n",
    "                    \"evidence_masks\": np.array([f.evidence_mask for f in features]),\n",
    "                    \"class_labels\": np.array([f.class_label for f in features]),\n",
    "                    \"input_masks\": np.array([f.input_mask for f in features]),\n",
    "                    \"p_masks\": np.array([f.p_mask for f in features]),\n",
    "                    }\n",
    "                with open(os.path.join(data_dir, 'human_annotations.pickle'), 'wb') as handle:\n",
    "                    pickle.dump(human_annotations, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        torch.save(features, cached_features_file)\n",
    "        tensorized_dataset = feature_to_tensor(features, rationale_level=rationale_level)\n",
    "\n",
    "    return tensorized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features from cached file %s data/movies/token/val/token_cached_features_file.pt\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1794/1150493806.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mconfigs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data_params\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#train_dataloader, valid_dataloader, test_dataloader = load_data(data_dir, configs['data_params'], tokenizer, SEED)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_split_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrationale_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rationale_level\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mval_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1794/1728391143.py\u001b[0m in \u001b[0;36mload_split_data\u001b[0;34m(data_dir, configs, split, seed, tokenizer, partial_train, rationale_level, save_human_annotation)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_features_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'overwrite_cache'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading features from cached file %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_features_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_features_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtensorized_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrationale_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrationale_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensorized_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs_678_final_project_environment_ubuntu/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs_678_final_project_environment_ubuntu/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs_678_final_project_environment_ubuntu/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1122\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mmod_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;31m# Load the data (which may in turn use `persistent_load` to load tensors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "# TODO: Alter this cell to load our data in the same format\n",
    "\n",
    "######## FROM ORIGINAL TUTORIAL ########\n",
    "\"\"\"\n",
    "from nlp import load_dataset\n",
    "qqp_data = load_dataset('glue', 'qqp', split='validation')\n",
    "all_questions = set()\n",
    "q1s = [d[\"question1\"] for d in qqp_data]\n",
    "q2s = [d[\"question2\"] for d in qqp_data]\n",
    "labels = np.array([d[\"label\"] for d in qqp_data]).astype(int)\n",
    "\n",
    "qs = list(zip(q1s, q2s))\n",
    "qqp_data[0]\n",
    "\"\"\"\n",
    "\n",
    "######## FOR OUR LIMITEDINK PROJECT ########\n",
    "\n",
    "from limitedink.utils import utils\n",
    "from limitedink.utils.utils_dataloader import set_seed, load_data\n",
    "import json\n",
    "\n",
    "SEED = 1234\n",
    "set_seed(SEED)\n",
    "data_dir = \"data/movies\"\n",
    "with open(\"limitedink/params/movies_config_token.json\", \"r\") as json_file:\n",
    "    configs = json.load(json_file)\n",
    "configs = configs[\"data_params\"]\n",
    "#train_dataloader, valid_dataloader, test_dataloader = load_data(data_dir, configs['data_params'], tokenizer, SEED)\n",
    "val_dataset = load_split_data(data_dir, configs, split=\"val\", seed=SEED, tokenizer=tokenizer, partial_train=1.0, rationale_level=configs[\"rationale_level\"])\n",
    "val_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset), batch_size = configs['batch_size'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess all the questions with spacy.  This may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'q1s' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_297/3657202419.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mall_questions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mall_questions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq2s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total count of unique questions: {len(all_questions)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprocessed_qs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_questions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'q1s' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "all_questions.update(set(q1s))\n",
    "all_questions.update(set(q2s))\n",
    "print(f\"Total count of unique questions: {len(all_questions)}\")\n",
    "processed_qs = list(tqdm(processor.pipe(all_questions, batch_size=64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_map = {q: processed_q for (q, processed_q) in zip(all_questions, processed_qs)}\n",
    "parsed_qs = [(spacy_map[q[0]], spacy_map[q[1]]) for q in qs]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the CheckList matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = TestSuite()\n",
    "editor = Editor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capability: Robustness\n",
    "> I think this is the only capability Antonis wants us to test.  -Wren"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MFT (Minimum Functionality Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the ground-truth sentiment of a review hinges on a single word, the output should match the sentiment of the word\n",
    "mft_template = editor.template((\n",
    "    \"This movie is {descriptor}.\"\n",
    "    ),\n",
    "    remove_duplicates=True,\n",
    "    nsamples=300\n",
    "    )\n",
    "test = MFT(**mft_template, labels=0, name=\"movie descriptors\", capability=\"robustness\",\n",
    "           description=\"straightforward descriptions of movies\")\n",
    "suite.add(test)\n",
    "# Show some example prompts\n",
    "print(mft_template.data[0])\n",
    "print(mft_template.data[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INV (Invariance Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introducing minor typos should not change the output label\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing pronoun genders should not change the output label\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing named entity names should not change the output label\n",
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DIR (Directional Expectation Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a single sentence that says \"This movie is good.\" at the end should not make the output label more negative.\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a single sentence that says \"This movie is bad.\" at the end should not make the output label more positive.\n",
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the suite, seeing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs_678_final_project_environment_ubuntu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
